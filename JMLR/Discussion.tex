\section{Discussion}
\label{sec:disc}

Regression trees have been introduced in the 1980s through the popular CART algorithm \cite{breiman1984classification}, notably allowing one to deal with both categorical and numerical input variables. They constitute the basic building block of state-of-the-art ensemble methods based on the combination of random trees, notably Random Forests introduced by Breiman in~\cite{breiman2001random} to circumvent the instability of Regression trees \cite{gey2006}. Since Random Forests are particularly well suited for Big Data analysis (see \cite{genuer2017}), many applications have been addressed in various domains with Random Forests, for example in ecology or genomics. \cite{verikas2011} provides a review of the use of Random Forests for data mining purposes. An interesting feature of Random Forests is the possibility to quantify the importance of input variables in the model obtained (see~\cite{genuer2010} for more details on that point). Another interesting feature, which was empirically established, is their robustness to noise. They are thus, to a certain extent, robust to uncertain inputs (even though no mechanism was specifically designed for that). This said, explicitly modelling the uncertainty as done in the uncertain regression trees proposed here allows one to outperform the standard version of  Random Forests, as illustrated in our experiments. 

Several adaptations of ensemble methods for quantile regression have been proposed, as quantile Random Forests or quantile boosting trees \cite{fenske2011identifying, kriegler2007boosting, kriegler2010small,meinshausen2006quantile,zheng2012}. These studies however focus on the uncertainty in the output variable (by producing conditional quantiles rather than a conditional mean) and not on the uncertainties in the inputs, as done here. It is of course possible to combine both approaches, which we intend to do in the future.

Lastly, the idea of including information on the uncertainty of the input data in a regression method is not new and is related to uncertainty propagation and sensitivity analysis. Several authors have indeed proposed to integrate uncertainties on the inputs in different regression methods, as multivariate linear models \cite{reis2005} or neural networks \cite{gal2016}. In each case, the methods have been improved, showing the benefits of explicitly modelling uncertainties in the input data. To the best of our knowledge, our approach is the first one to address this problem in the context of regression trees (and ensemble methods based on such trees). Our conclusion on the benefits of this approach parallels the ones of the above-mentioned studies.
