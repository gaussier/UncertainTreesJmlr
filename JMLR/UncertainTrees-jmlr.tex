\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
%\usepackage{amsthm}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{pgfplots}
\usepackage{mathdots}
\usepackage{multirow}
%\usepackage[lined,ruled,vlined,boxed]{algorithm2e}
\usepackage[vlined,ruled,linesnumbered]{algorithm2e}
%\usepackage{balance}
%\setcopyright{acmcopyright}

\usepackage[textsize=tiny]{todonotes}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,dsfont,amssymb}
\usepackage{ctable}
\usepackage[graphicx]{realboxes}
\usepackage{arydshln}

\usepackage{xcolor}
\usepackage{soul}

% Definitions of handy macros can go here
\newcommand{\inputTikZ}[2]{%  
     \scalebox{#1}{\input{#2}}  
}

%\newcommand{\dataset}{{\cal D}}
%\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
%\newcommand{\LDA}{\texttt{LDA}}
%\newcommand{\LDAo}{\texttt{LDA}$_1$}
%\newcommand{\LDAall}{\texttt{LDA}$_{all}$}
%\newcommand{\DTM}{\texttt{DTM}}
%\newcommand{\DMM}{\texttt{DMM}}
%\newcommand{\STLDA}{\texttt{ST-LDA}}
%\newcommand{\TTM}{\texttt{TTM}}
%\newcommand{\cDTM}{\texttt{cTDM}}
%\newcommand{\iDTM}{\texttt{iDTM}}
%\newcommand{\dHDP}{\texttt{dHDP}}
%\newcommand{\TDPM}{\texttt{TDPM}}
%\newcommand{\EDP}{\texttt{EDP}}
%\newcommand{\EHDP}{\texttt{EHDP}}
%\newcommand{\TMLDA}{\texttt{TM-LDA}}
%\newcommand{\Modeld}{\texttt{ST-LDA-D}}
%\newcommand{\Modelc}{\texttt{ST-LDA-C}}
%\newcommand{\Modeldc}{\texttt{ST-LDA-[D|C]}} 
%\newcommand{\NIPS}{\textsf{NIPS}}
%\newcommand{\TDT}{\textsf{TDT4}}
%\newcommand{\Tweets}{\textsf{Tweets}}
%\newcommand{\NYT}{\textsf{NYT}}
%\newcommand{\Tech}{\textsf{Tech}}
%\newcommand{\HDP}{\textsf{HDP}}
%\newcommand{\HDPS}{\textsf{HDPs}}
%\newcommand{\ILDA}{\textsf{iLDA}}
%\newcommand{\CopHDP}{\textsf{CopHDP}}

\newcommand{\Emilie}[1]{\todo[inline,backgroundcolor=orange!20!white]{Emilie: #1}}
\newcommand{\Myriam}[1]{\todo[inline,backgroundcolor=yellow!20!white]{Myriam: #1}}
\newcommand{\Marianne}[1]{\todo[inline,backgroundcolor=green!20!white]{Marianne: #1}}
\newcommand{\Eric}[1]{\todo[inline,backgroundcolor=blue!20!white]{Eric: #1}}

\firstpageno{1}

\begin{document}

\title{Uncertain Trees: Dealing with Uncertain Inputs in Regression Trees}

\author{\name Myriam Tami Amoualian \email myriam.tami@univ-grenoble-alpes.fr\\
       \addr Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France
       \AND
       \name Emilie Devijver \email emilie.devijver@univ-grenoble-alpes.fr\\
       \addr Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France
       \AND
       \name Marianne Clausel \email marianne.clausel@univ-lorraine.fr\\
       \addr University of Lorraine Nancy, IEC, 54052 Nancy, France
       \AND
       \name Eric Gaussier \email eric.gaussier@univ-grenoble-alpes.fr\\
       \addr Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France
       }

\editor{}

\maketitle

\begin{abstract}
Tree-based ensemble methods, as Random Forests and Gradient Boosted Trees, have been successfully used for regression in many applications and research studies. Furthermore, these methods have been extended in order to deal with uncertainty in the output variable, using for example a quantile loss in Random Forests \cite{meinshausen2006quantile}. To the best of our knowledge, no extension has been provided yet for dealing with uncertainties in the input variables, even though such uncertainties are common in practical situations. We propose here such an extension by showing how standard regression trees optimizing a quadratic loss can be adapted and learned while taking into account the uncertainties in the inputs. By doing  so, one no longer assumes that an observation lies into a single region of the regression tree, but rather that it belongs to each region with a certain probability. Experiments conducted on several data sets illustrate the good behavior of the proposed extension.
\end{abstract}

\begin{keywords}
Regression trees, Random Forests, Uncertain input variables
\end{keywords}

\input{Introduction}
\input{Model-Algos}
\input{Theory-RFs}
\input{Experiments}
\input{Discussion}
\input{Conclusion}
\vskip 0.2in

\bibliography{ref}

\end{document}
