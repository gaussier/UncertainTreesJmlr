\section{Introduction}

Most measures carried out in the real world, \textit{e.g.}, by sensors embedded in different machines or by analyses of samples, are uncertain if not erroneous in some cases. This uncertainty may be due to the generating process of the samples being measured or from the intrinsic limitations of any measurement process. Considering such measures, that constitute many of the data sets used in data science applications in both industry and academy, as certain is thus in the best case a naive position. Our experiments illustrate this point inasmuch as the method we propose here to handle uncertainty outperforms standard approaches on several, real benchmark data sets.

However, if several methods have been developed to obtain uncertainty estimates from data sets, very few studies have been devoted to designing data science methods that can deal with such uncertainties. We address this problem here in the context of regression trees, a popular machine learning method at the basis of widely used ensemble methods as Random Forests.

In this context, recent studies have focused on providing conditional quantiles, as opposed to conditional means, so as to better represent the output variable and avoid the uncertainty inherent to point estimates. The work on quantile regression forests developed by Meinshausen \cite{meinshausen2006quantile} is a good illustration of this. We take here a different approach and directly model the uncertainty of the input variables in the regression trees we consider. This leads to a regression tree in which observations no longer belong to a single leaf. Instead, each observation has a non-null probability of being assigned to any leaf of the tree. The construction process associated to such trees is slightly more complex than the ones of standard trees (it involves in particular the inversion of a $K \times K$ matrix, where $K$ is the number of leaves of the tree), but the improvements obtained on the prediction fully justify this additional complexity, as shown in Section~\ref{sec:exps} on both benchmark and modified (with an additional noise) data sets.

The idea of including information on the uncertainty of the input data in a regression method is not new and is related to uncertainty propagation and sensitivity analysis. Several authors have indeed proposed to integrate uncertainties on the inputs in different regression methods, as multivariate linear models \cite{reis2005} or neural networks \cite{gal2016}. To the best of our knowledge, our approach is the first one to address this problem in the context of regression trees and ensemble methods based on such trees, as Random Forests.

The remainder of the paper is organized as follows: Section~\ref{sec:model} presents the general model we rely on and its main properties; Section~\ref{sec:algo} then describes the algorithms for constructing the regression tree and the associated prediction rule, while Section~\ref{sec:exps} presents the experiments conducted and the numerical results obtained. Finally, Section~\ref{sec:disc} positions our work wrt previous studies while Section~\ref{sec:concl} concludes the paper.

